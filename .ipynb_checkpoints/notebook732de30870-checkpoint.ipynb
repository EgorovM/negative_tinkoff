{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_texts = pd.read_csv('../input/russian-language-toxic-comments/labeled.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\nimport pymorphy2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words = stopwords.words('russian')\nmorph = pymorphy2.MorphAnalyzer()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(comment):\n    comment = comment.lower()\n\n    comment = re.sub(r\"@[а-яА-ЯA-Za-z0-9]+\", ' ', comment)\n\n    comment = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', comment)\n    comment = re.sub(r\"http?://[A-Za-z0-9./]+\", ' ', comment)\n        \n    comment = re.sub(r\"[^а-яА-Яa-zA-Z.!?']\", ' ', comment)\n\n    comment = re.sub(r\" +\", ' ', comment)\n    \n    for ch in string.punctuation:\n        comment = comment.replace(ch, ' ')\n        \n    return \" \".join([morph.parse(word)[0].normal_form for word in comment.split() if not word in stop_words])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_texts['clean_comment'] = df_texts.comment.apply(clean_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer = TfidfVectorizer(max_features=2000)\nvectors = vectorizer.fit_transform(df_texts['clean_comment'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(vectors, df_texts['toxic'], test_size=0.3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = RandomForestClassifier()\nmodel.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_score(model.predict(X_test), y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    %tensorflow_version 2.x\nexcept Exception:\n    pass\nimport tensorflow as tf\n\nimport tensorflow_hub as hub\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import load_model\nimport bert\nfrom transformers import AutoTokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from bert import tokenization","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FullTokenizer = tokenization.FullTokenizer\n#FullTokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\nbert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n                            trainable=False)\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = FullTokenizer(vocab_file, do_lower_case)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode_sentence(sent):\n    return [\"[CLS]\"] + tokenizer.tokenize(sent)[:510] + [\"[SEP]\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_inputs = [encode_sentence(sentence) for sentence in df_texts['clean_comment']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokens = set()\nfor text in data_inputs:\n    for token in text[1:-1]:\n        tokens.add(token)\ntokens = sorted(tokens)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}